{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    156060.00000\n",
       "mean          6.89463\n",
       "std           6.57485\n",
       "min           0.00000\n",
       "25%           2.00000\n",
       "50%           4.00000\n",
       "75%           9.00000\n",
       "max          48.00000\n",
       "Name: phracelen, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './Tomatoes/'\n",
    "\n",
    "data = pd.read_csv(\"%s%s\" % (data_path, 'train.tsv'), sep = '\\t')\n",
    "test_data = pd.read_csv(\"%s%s\" % (data_path, 'test.tsv'), sep = '\\t')\n",
    "\n",
    "import re\n",
    "def clean(_str):\n",
    "    return \" \".join(re.findall(\"[0-9a-zA-Z]*\", _str)).strip()\n",
    "def split(_str):\n",
    "    return _str.split()\n",
    "\n",
    "data['Phrase'] = data['Phrase'].apply(clean)\n",
    "test_data['Phrase'] = data['Phrase'].apply(clean)\n",
    "\n",
    "def _len(_str):\n",
    "    return len(_str.split())\n",
    "data['phracelen'] = data['Phrase'].apply(_len)\n",
    "data['phracelen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124848, 5), (31212, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, train_size = 0.8, random_state = 22)\n",
    "\n",
    "for train_index, dev_index in split.split(data, data[['Sentiment']]):\n",
    "    dev_data = data.loc[dev_index]\n",
    "    train_data = data.loc[train_index]\n",
    "train_data.shape, dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_w2v(splited_corpus, w2v_size, min_count):\n",
    "    '''\n",
    "    func: 获取word2vec模型\n",
    "    param: splited_corpus\n",
    "        type: pd.Series\n",
    "        detail: 应当为训练集中所有语料\n",
    "    param: w2v_size\n",
    "        type: int\n",
    "        detail: w2v向量维度\n",
    "    return: w2v_model\n",
    "        type: gensim.models.Word2Vec\n",
    "        detail: 训练的模型只可以使用其transform接口\n",
    "    '''\n",
    "    sentences = [x.split() for x in splited_corpus]\n",
    "    model = gensim.models.Word2Vec(sentences, min_count=min_count, size=w2v_size)\n",
    "    return model\n",
    "\n",
    "def get_w2v_key_vev(w2v_model):\n",
    "    vecs = []\n",
    "    words = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        vecs.append(w2v_model[word])\n",
    "        words.append(word)\n",
    "    return words, vecs\n",
    "\n",
    "def get_x_index(x, words):\n",
    "    res = []\n",
    "    for inst in x:\n",
    "        res.append(np.array([words.index(word) for word in inst.split() if word in words]))\n",
    "    return res\n",
    "\n",
    "def max_len(list_2d):\n",
    "    maxlen = 0\n",
    "    for arr in list_2d:\n",
    "        if(len(arr) > maxlen):\n",
    "            maxlen = len(arr)\n",
    "    return maxlen\n",
    "\n",
    "def mean_len(list_2d):\n",
    "    mean_len = 0\n",
    "    for arr in list_2d:\n",
    "        mean_len += len(arr)\n",
    "    return int(mean_len / len(list_2d))\n",
    "\n",
    "def ceil2(num):\n",
    "    res = 2\n",
    "    while res < num:\n",
    "        res *= 2\n",
    "    return res\n",
    "\n",
    "def padding(data2d, max_len, pad_val):\n",
    "    res = []\n",
    "    for index, seq in enumerate(data2d):\n",
    "        if(len(seq) < max_len):\n",
    "            res.append(np.concatenate([seq, np.full([max_len - len(seq)], pad_val)]))\n",
    "        else:\n",
    "            res.append(seq[:max_len])\n",
    "    return res\n",
    "\n",
    "def concat_list_h(list1, list2):\n",
    "    res = []\n",
    "    for i, ele in enumerate(list1):\n",
    "        res.append(np.concatenate([ele, list2[i]]))\n",
    "    return res\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()\n",
    "train_y = np.array(list(train_data['Sentiment'])).reshape(-1, 1)\n",
    "train_y = oh_enc.fit_transform(train_y).toarray()\n",
    "dev_y = np.array(list(dev_data['Sentiment'])).reshape(-1, 1)\n",
    "dev_y = oh_enc.fit_transform(dev_y).toarray()\n",
    "corpus = list(train_data['Phrase'])\n",
    "max_seq_len = ceil2(max_len(corpus))\n",
    "mean_seq_len = mean_len(corpus)\n",
    "print(max_seq_len, mean_seq_len)\n",
    "max_seq_len = 16\n",
    "w2v_model = get_w2v(corpus, 300, min_count = 1)\n",
    "words, embedding_matrix = get_w2v_key_vev(w2v_model)\n",
    "embedding_matrix.append([0 for i in range(len(embedding_matrix[0]))])\n",
    "\n",
    "train_x = get_x_index(list(train_data['Phrase']), words)\n",
    "train_x = padding(train_x, max_seq_len, len(embedding_matrix) - 1)\n",
    "\n",
    "dev_x = get_x_index(list(dev_data['Phrase']), words)\n",
    "dev_x = padding(dev_x, max_seq_len, len(embedding_matrix) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-bccd74e272cf>:89: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-4-bccd74e272cf>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-bccd74e272cf>:107: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "tf log dir :  tf_logs/run-20191116235501/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "from datetime import datetime\n",
    "\n",
    "class CNNLSTM:\n",
    "    def __init__(self, seq_len, num_classes, batch_seqs_num, embedding_matrix, embedding_size, filter_sizes,\n",
    "                num_filters, conv_activate_fn = tf.nn.relu, fcl_activate_fn = tf.sigmoid, learning_rate = 0.01,\n",
    "                n_epochs = 100, filtered_dims = 128, pooled_dims = 100, num_lstm_cells = 1, lstm_hiden_size = 32,\n",
    "                cnn_drop_out_prob = 0.5, lstm_drop_out_prob = 0.5, sum_root_dir = \"tf_logs\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_seqs_num = batch_seqs_num\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_activate_fn = conv_activate_fn\n",
    "        self.fcl_activate_fn = fcl_activate_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.filtered_dims = filtered_dims\n",
    "        self.pooled_dims = pooled_dims\n",
    "        if(filtered_dims > embedding_size):\n",
    "            print('filtered_dims should be less than embedding_size')\n",
    "        self.num_lstm_cells = num_lstm_cells\n",
    "        self.lstm_hiden_size = lstm_hiden_size\n",
    "        self.cnn_drop_out_prob = cnn_drop_out_prob\n",
    "        self.lstm_drop_out_prob = lstm_drop_out_prob\n",
    "        self.log_dir = self.log_dir(sum_root_dir)\n",
    "        self.graph = tf.Graph()\n",
    "    \n",
    "    def log_dir(self, root_logdir):\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "        return log_dir\n",
    "    \n",
    "    def build(self):\n",
    "        with tf.name_scope(\"cnn_lstm\"), self.graph.as_default():\n",
    "            self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, self.seq_len], name = \"input_x\")\n",
    "            self.input_y = tf.placeholder(dtype = tf.int32, shape = [None, self.num_classes], name = \"input_y\")\n",
    "            self.cnn_dropout_keep_prob = tf.placeholder(tf.float32, name=\"cnn_dropout_keep_prob\")\n",
    "            self.lstm_dropout_keep_prob = tf.placeholder(tf.float32, name=\"lstm_dropout_keep_prob\")\n",
    "            self.global_step = tf.Variable(0, trainable = False)\n",
    "\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.embedding_matrix = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "                self.embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, self.input_x), -1,\n",
    "                                               name = \"embedded_output\")\n",
    "            self.pooled_outputs = []\n",
    "            filter_size = self.filter_sizes[0]\n",
    "            with tf.name_scope(\"conv_maxpool\"):\n",
    "                filter_shape = [filter_size, self.embedding_size - self.filtered_dims + 1, 1, self.num_filters]\n",
    "                filter_weight = tf.Variable(tf.truncated_normal(filter_shape, -1, 1), name = \"filter_weight\")\n",
    "                filter_bias = tf.Variable(tf.truncated_normal([self.num_filters], -1, 1))\n",
    "                conv_output = self.conv_activate_fn(tf.nn.bias_add(tf.nn.conv2d(\n",
    "                    self.embedded,\n",
    "                    filter_weight,\n",
    "                    strides = [1, 1, 1, 1],\n",
    "                    padding = 'VALID',\n",
    "                ), filter_bias, name = \"conv_output\"), name = \"act_conv_output\")\n",
    "\n",
    "                pooled_output = tf.nn.max_pool(conv_output,\n",
    "                                              ksize = [1, 1, self.filtered_dims - self.pooled_dims + 1, 1],\n",
    "                                              strides = [1, 1, 1, 1],\n",
    "                                              padding = \"VALID\",\n",
    "                                              name = \"pooled_output\")\n",
    "                self.reduced_pooled_output = tf.reshape(pooled_output, [-1, pooled_output.shape[1],\n",
    "                                                                            (self.pooled_dims) * self.num_filters],\n",
    "                                                       name = 'reduced_pooled_output')\n",
    "            with tf.name_scope(\"cnn_dropout\"):\n",
    "                self.droped_pooled_output = tf.nn.dropout(self.reduced_pooled_output, self.cnn_dropout_keep_prob)\n",
    "\n",
    "                #self.pooled_outputs.append(pooled_output)\n",
    "            #拼接\n",
    "            #self.total_filters_num = self.num_filters * len(self.filter_sizes)\n",
    "           #print(self.total_filters_num)\n",
    "            #self.concated_output = tf.concat(self.pooled_outputs, -1)\n",
    "            #print(self.concated_output.shape)\n",
    "            #self.reduced_output = tf.reshape(self.concated_output, [-1, self.seq_len - filter_size + 1, \n",
    "                                                                                 #self.filtered_dims - self.pool_minus_dims], \n",
    "                                                      #name = \"conv_maxpool_output\")\n",
    "            with tf.name_scope(\"lstm\"):\n",
    "                self.lstm_cells = [tf.nn.rnn_cell.BasicLSTMCell(self.lstm_hiden_size, name = \"%s%d\" % ('lstmcell_', i)) \n",
    "                                  for i in range(self.num_lstm_cells)]\n",
    "                self.cells = tf.nn.rnn_cell.MultiRNNCell(self.lstm_cells)\n",
    "                self.initial_state = self.cells.zero_state(self.batch_seqs_num, tf.float32)\n",
    "                self.lstm_output, self.lstm_state = tf.nn.dynamic_rnn(self.cells, self.reduced_pooled_output, \n",
    "                                                                     dtype = tf.float32)\n",
    "                self.lstm_last_output = self.lstm_state[-1].h\n",
    "            with tf.name_scope(\"lstm_dropout\"):\n",
    "                self.droped_lstm_output = tf.nn.dropout(self.lstm_last_output, self.lstm_dropout_keep_prob)\n",
    "            with tf.name_scope(\"full_connect\"):\n",
    "                self.fcl_output = tf.contrib.layers.fully_connected(self.droped_lstm_output, \n",
    "                                                              self.num_classes,\n",
    "                                                              self.fcl_activate_fn)\n",
    "            self.prediction = self.fcl_output\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits = self.prediction, labels = self.input_y,  \n",
    "                ))\n",
    "            with tf.name_scope(\"target\"):\n",
    "                self.train_correct = tf.equal(tf.arg_max(self.prediction, 1), tf.arg_max(self.input_y, 1), name = \"correct\")\n",
    "                self.train_acc = tf.reduce_mean(tf.cast(self.train_correct, tf.float32), name = \"acc\")\n",
    "            with tf.name_scope(\"summary\"):\n",
    "                self.loss_sum = tf.summary.scalar(\"loss\", self.loss)\n",
    "                self.acc_sum = tf.summary.scalar(\"acc\", self.train_acc)\n",
    "                self.sum = tf.summary.merge_all()\n",
    "                self.filewriter = tf.summary.FileWriter(self.log_dir, tf.get_default_graph())\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.loss, global_step = self.global_step)\n",
    "            \n",
    "    \n",
    "    def fit(self, x, y, dev_x = None, dev_y = None, test_x = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.dev_x = dev_x\n",
    "        self.dev_y = dev_y\n",
    "        self.test_x = test_x\n",
    "    \n",
    "    def train(self):\n",
    "        print('tf log dir : ', self.log_dir)\n",
    "        n_batches = int(np.ceil(len(self.x) / self.batch_seqs_num))\n",
    "        batch_size = self.batch_seqs_num\n",
    "        dev_feed_dict = {\n",
    "            self.input_x : self.dev_x,\n",
    "            self.input_y : self.dev_y,\n",
    "            self.cnn_dropout_keep_prob : 1.0,\n",
    "            self.lstm_dropout_keep_prob : 1.0\n",
    "        }\n",
    "        train_feed_dict = {\n",
    "            self.input_x : self.x[:10000],\n",
    "            self.input_y : self.y[:10000],\n",
    "            self.cnn_dropout_keep_prob : 1.0,\n",
    "            self.lstm_dropout_keep_prob : 1.0\n",
    "        }\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config = config, graph = self.graph) as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.input_x : batch_x,\n",
    "                        self.input_y : batch_y,\n",
    "                        self.cnn_dropout_keep_prob : self.cnn_drop_out_prob,\n",
    "                        self.lstm_dropout_keep_prob : self.lstm_drop_out_prob\n",
    "                    }\n",
    "                    sess.run(self.train_step, feed_dict = feed_dict)\n",
    "                    step = epoch * n_batches + batch_index\n",
    "                print('train epoch %d / %d Done' % (epoch, self.n_epochs))\n",
    "                if(epoch % 5 == 0):\n",
    "                    print('dev acc', self.train_acc.eval(feed_dict = dev_feed_dict))\n",
    "                    print('train loss', self.loss.eval(feed_dict = train_feed_dict))\n",
    "                    dev_acc_str = self.acc_sum.eval(feed_dict = dev_feed_dict)\n",
    "                    train_loss_str = self.loss_sum.eval(feed_dict = train_feed_dict)\n",
    "                    self.filewriter.add_summary(dev_acc_str, step)\n",
    "                    self.filewriter.add_summary(train_loss_str, step)\n",
    "\n",
    "m = CNNLSTM(max_seq_len, 5, 32, np.array(embedding_matrix), len(embedding_matrix[0]), [3], 8, learning_rate = 0.0001,\n",
    "            n_epochs = 1000, num_lstm_cells = 2)\n",
    "m.build()\n",
    "m.fit(train_x, train_y, dev_x, dev_y)\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-3eee0271986a>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3eee0271986a>\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    with tf.name_scope(\"full_connect\"):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "\n",
    "class CNNLSTM:\n",
    "    def __init__(self, seq_len, num_classes, batch_seqs_num, embedding_matrix, embedding_size, filter_sizes,\n",
    "                num_filters, conv_activate_fn = tf.nn.relu, fcl_activate_fn = tf.sigmoid, learning_rate = 0.01,\n",
    "                n_epochs = 100, filtered_dims = 10, pool_minus_dims = 5):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_seqs_num = batch_seqs_num\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_activate_fn = conv_activate_fn\n",
    "        self.fcl_activate_fn = fcl_activate_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.filtered_dims = 10\n",
    "        self.pool_minus_dims = pool_minus_dims\n",
    "        if(filtered_dims > embedding_size):\n",
    "            print('filtered_dims should be less than embedding_size')\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, self.seq_len], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(dtype = tf.int32, shape = [None, self.num_classes], name = \"input_y\")\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embedding_matrix = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "            self.embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, self.input_x), -1,\n",
    "                                           name = \"embedded_output\")\n",
    "        self.pooled_outputs = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.name_scope(\"conv_maxpool\"):\n",
    "                filter_shape = [filter_size, self.embedding_size - self.filtered_dims + 1, 1, self.num_filters]\n",
    "                filter_weight = tf.Variable(tf.truncated_normal(filter_shape, -1, 1), name = \"filter_weight\")\n",
    "                filter_bias = tf.Variable(tf.truncated_normal([self.num_filters], -1, 1))\n",
    "                conv_output = self.conv_activate_fn(tf.nn.bias_add(tf.nn.conv2d(\n",
    "                    self.embedded,\n",
    "                    filter_weight,\n",
    "                    strides = [1, 1, 1, 1],\n",
    "                    padding = 'VALID',\n",
    "                ), filter_bias, name = \"conv_output\"), name = \"act_conv_output\")\n",
    "                pooled_output = tf.nn.max_pool(conv_output,\n",
    "                                              ksize = [1, self.filtered_dims - self.pool_minus_dims + 1, 1, 1],\n",
    "                                              strides = [1, 1, 1, 1],\n",
    "                                              padding = \"VALID\",\n",
    "                                              name = \"pooled_output\")\n",
    "                self.pooled_outputs.append(pooled_output)\n",
    "        #拼接\n",
    "        self.total_filters_num = self.num_filters * len(self.filter_sizes)\n",
    "        self.concated_output = tf.concat(self.pooled_outputs, 3)\n",
    "        self.reduced_concated_output = tf.reshape(self.concated_output, [-1, self.max_seq_len - filter_size + 1, \n",
    "                                                                             self.filtered_dims - self.pool_minus_dims], \n",
    "                                                  name = \"conv_maxpool_output\")\n",
    "        with tf.name_scope(\"lstm\"):\n",
    "            \n",
    "        with tf.name_scope(\"full_connect\"):\n",
    "            self.fcl_output = tf.contrib.layers.fully_connected(self.reduced_concated_output, \n",
    "                                                          self.num_classes,\n",
    "                                                          self.fcl_activate_fn)\n",
    "        self.prediction = self.fcl_output\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = self.prediction, labels = self.input_y,  \n",
    "            ))\n",
    "        with tf.name_scope(\"target\"):\n",
    "            self.train_correct = tf.equal(tf.arg_max(self.prediction, 1), tf.arg_max(self.input_y, 1), name = \"correct\")\n",
    "            self.train_acc = tf.reduce_mean(tf.cast(self.train_correct, tf.float32), name = \"acc\")\n",
    "            #self.train_f1 = tf.contrib.metrics.f1_score(tf.arg_max(self.prediction, 1), tf.arg_max(self.input_y, 1), name = \"f1\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "    \n",
    "    def fit(self, x, y, dev_x = None, dev_y = None, test_x = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.dev_x = dev_x\n",
    "        self.dev_y = dev_y\n",
    "        self.test_x = test_x\n",
    "    \n",
    "    def train(self):\n",
    "        n_batches = int(np.ceil(len(self.x) / self.batch_seqs_num))\n",
    "        batch_size = self.batch_seqs_num\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.input_x : batch_x,\n",
    "                        self.input_y : batch_y\n",
    "                    }\n",
    "                    sess.run(self.train_step, feed_dict = feed_dict)\n",
    "                if(epoch % 5 == 0):\n",
    "                    print('loss', self.loss.eval(feed_dict = feed_dict))\n",
    "                    print('train acc', self.train_acc.eval(feed_dict = feed_dict))\n",
    "            \n",
    "            \n",
    "\n",
    "m = typical(max_seq_len, 2, 32, np.array(embedding_matrix), len(embedding_matrix[0]), [3], 1, learning_rate = 0.001,\n",
    "            n_epochs = 1000)\n",
    "m.build()\n",
    "m.fit(train_x, train_y)\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "\n",
    "class CNNLSTM:\n",
    "    def __init__(self, seq_len, num_classes, batch_seqs_num, embedding_matrix, embedding_size, filter_sizes,\n",
    "                num_filters, conv_activate_fn = tf.nn.relu, fcl_activate_fn = tf.sigmoid, learning_rate = 0.01,\n",
    "                n_epochs = 100, filtered_dims = 10, pool_minus_dims = 5):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_seqs_num = batch_seqs_num\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.conv_activate_fn = conv_activate_fn\n",
    "        self.fcl_activate_fn = fcl_activate_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.filtered_dims = 10\n",
    "        self.pool_minus_dims = pool_minus_dims\n",
    "        if(filtered_dims > embedding_size):\n",
    "            print('filtered_dims should be less than embedding_size')\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, self.seq_len], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(dtype = tf.int32, shape = [None, self.num_classes], name = \"input_y\")\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embedding_matrix = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "            self.embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix, self.input_x), -1,\n",
    "                                           name = \"embedded_output\")\n",
    "        self.pooled_outputs = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.name_scope(\"conv_maxpool\"):\n",
    "                filter_shape = [filter_size, self.embedding_size - self.filtered_dims + 1, 1, self.num_filters]\n",
    "                filter_weight = tf.Variable(tf.truncated_normal(filter_shape, -1, 1), name = \"filter_weight\")\n",
    "                filter_bias = tf.Variable(tf.truncated_normal([self.num_filters], -1, 1))\n",
    "                conv_output = self.conv_activate_fn(tf.nn.bias_add(tf.nn.conv2d(\n",
    "                    self.embedded,\n",
    "                    filter_weight,\n",
    "                    strides = [1, 1, 1, 1],\n",
    "                    padding = 'VALID',\n",
    "                ), filter_bias, name = \"conv_output\"), name = \"act_conv_output\")\n",
    "                pooled_output = tf.nn.max_pool(conv_output,\n",
    "                                              ksize = [1, self.filtered_dims - self.pool_minus_dims + 1, 1, 1],\n",
    "                                              strides = [1, 1, 1, 1],\n",
    "                                              padding = \"VALID\",\n",
    "                                              name = \"pooled_output\")\n",
    "                self.pooled_outputs.append(pooled_output)\n",
    "        #拼接\n",
    "        self.total_filters_num = self.num_filters * len(self.filter_sizes)\n",
    "        self.concated_output = tf.concat(self.pooled_outputs, 3)\n",
    "        self.reduced_concated_output = tf.reshape(self.concated_output, [-1, self.max_seq_len - filter_size + 1, \n",
    "                                                                             self.filtered_dims - self.pool_minus_dims], \n",
    "                                                  name = \"conv_maxpool_output\")\n",
    "        with tf.name_scope(\"lstm\"):\n",
    "            \n",
    "        with tf.name_scope(\"full_connect\"):\n",
    "            self.fcl_output = tf.contrib.layers.fully_connected(self.reduced_concated_output, \n",
    "                                                          self.num_classes,\n",
    "                                                          self.fcl_activate_fn)\n",
    "        self.prediction = self.fcl_output\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = self.prediction, labels = self.input_y,  \n",
    "            ))\n",
    "        with tf.name_scope(\"target\"):\n",
    "            self.train_correct = tf.equal(tf.arg_max(self.prediction, 1), tf.arg_max(self.input_y, 1), name = \"correct\")\n",
    "            self.train_acc = tf.reduce_mean(tf.cast(self.train_correct, tf.float32), name = \"acc\")\n",
    "            #self.train_f1 = tf.contrib.metrics.f1_score(tf.arg_max(self.prediction, 1), tf.arg_max(self.input_y, 1), name = \"f1\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "    \n",
    "    def fit(self, x, y, dev_x = None, dev_y = None, test_x = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.dev_x = dev_x\n",
    "        self.dev_y = dev_y\n",
    "        self.test_x = test_x\n",
    "    \n",
    "    def train(self):\n",
    "        n_batches = int(np.ceil(len(self.x) / self.batch_seqs_num))\n",
    "        batch_size = self.batch_seqs_num\n",
    "        with tf.Session() as sess:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.input_x : batch_x,\n",
    "                        self.input_y : batch_y\n",
    "                    }\n",
    "                    tf.global_variables_initializer().run()\n",
    "                    sess.run(self.train_step, feed_dict = feed_dict)\n",
    "                if(epoch % 5 == 0):\n",
    "                    print('loss', self.loss.eval(feed_dict = feed_dict))\n",
    "                    print('train acc', self.train_acc.eval(feed_dict = feed_dict))\n",
    "            \n",
    "            \n",
    "\n",
    "m = typical(max_seq_len, 2, 32, np.array(embedding_matrix), len(embedding_matrix[0]), [3], 1, learning_rate = 0.001,\n",
    "            n_epochs = 1000)\n",
    "m.build()\n",
    "m.fit(train_x, train_y)\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
