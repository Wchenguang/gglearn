{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class lr_find():\n",
    "    def __init__(self, input_dims, num_classes, learning_rate_base = 1e-8, lr_gap = 1e-8, \n",
    "                 max_learning_rate = 1):\n",
    "        self.input_dims = input_dims\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.lr_gap = lr_gap\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        self.n_epochs = (max_learning_rate - learning_rate_base) / lr_gap\n",
    "        print(self.n_epochs)\n",
    "        \n",
    "    def build(self):\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.input_x = tf.place_holder(shape = [None, self.input_dims], dtype = tf.float32, name = \n",
    "                                          'input_x')\n",
    "            self.input_y = tf.place_holder(shape = [None, self.num_classes], dtype = tf.int32, name = \n",
    "                                          'input_y')\n",
    "            self.learning_rate = tf.place_holder(shape = [1], dtype = tf.float32, name = 'learning_rate')\n",
    "            \n",
    "        self.output = tf.contrib.layers.fully_connected(self.input_x, self.num_classes, None, name_scope = \"fcl\",\n",
    "                                                  name = \"output\")\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.output, labels = self.input_y), name = \"cross_entrophy\")\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.optimizer.minimize(loss)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.m = x.shape[0]\n",
    "        \n",
    "    def run(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class lr_find():\n",
    "    def __init__(self, input_dims, num_classes, embedding_matrix, learning_rate_base = 1e-8, lr_gap = 1e-8, \n",
    "                 max_learning_rate = 1):\n",
    "        self.input_dims = input_dims\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.lr_gap = lr_gap\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "\n",
    "        \n",
    "    def build(self):\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.input_x = tf.place_holder(shape = [None, self.input_dims], dtype = tf.float32, name = \n",
    "                                          'input_x')\n",
    "            self.input_y = tf.place_holder(shape = [None, self.num_classes], dtype = tf.int32, name = \n",
    "                                          'input_y')\n",
    "            self.learning_rate = tf.place_holder(shape = [1], dtype = tf.float32, name = 'learning_rate')\n",
    "            \n",
    "        self.output = tf.contrib.layers.fully_connected(self.input_x, self.num_classes, None, name_scope = \"fcl\",\n",
    "                                                  name = \"output\")\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                    tf.random_uniform(self.embedding_matrix.shape, -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "            self.embedding_matrix_ = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "            self.embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix_, self.input_x), -1,\n",
    "                                           name = \"embedded_output\")\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.output, labels = self.input_y), name = \"cross_entrophy\")\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_step = self.optimizer.minimize(loss)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.m = x.shape[0]\n",
    "        \n",
    "    def run(self):\n",
    "        self.n_batches_all = (max_learning_rate - learning_rate_base) / lr_gap\n",
    "        print(self.n_batches_all)\n",
    "        self.n_batches = self.m / self.batch_size\n",
    "        self.n_epochs = self.n_batches_all / self.n_batches}\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            self.learning_rate_val = self.learning_rate_base\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    if(self.learning_rate_val < self.max_learning_rate):\n",
    "                        self.learning_rate_val += self.lr_gap\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.input_x : batch_x,\n",
    "                        self.input_y : batch_y,\n",
    "                        self.learning_rate : self.learning_rate_val\n",
    "                    }\n",
    "                    print(feed_dict)\n",
    "                    #sess.run(self.train_step, feed_dict = feed_dict)\n",
    "    \n",
    "\n",
    "        \n",
    "lrf = lr_find(train_x.shape[1], 10, embedding_matrix)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class lr_find():\n",
    "    def __init__(self, input_dims, num_classes, embedding_matrix, learning_rate_base = 1e-8, lr_gap = 1e-8, \n",
    "                 max_learning_rate = 1):\n",
    "        self.input_dims = input_dims\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.lr_gap = lr_gap\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "\n",
    "        \n",
    "    def build(self):\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.input_x = tf.placeholder(shape = [None, self.input_dims], dtype = tf.int32, name = \n",
    "                                          'input_x')\n",
    "            self.input_y = tf.placeholder(shape = [None, self.num_classes], dtype = tf.int32, name = \n",
    "                                          'input_y')\n",
    "            self.learning_rate = tf.placeholder(shape = [1], dtype = tf.float32, name = 'learning_rate')\n",
    "            \n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.embedding_matrix_ = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "            self.embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embedding_matrix_, self.input_x), -1,\n",
    "                                           name = \"embedded_output\")\n",
    "            \n",
    "        self.output = tf.contrib.layers.fully_connected(self.embedded, self.num_classes, None)\n",
    "        print(self.output.shape)\n",
    "        print(self.input_y.shape)\n",
    "        \n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.output, labels = self.input_y), name = \"cross_entrophy\")\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_step = self.optimizer.minimize(loss)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.m = x.shape[0]\n",
    "        \n",
    "    def run(self):\n",
    "        self.n_batches_all = (max_learning_rate - learning_rate_base) / lr_gap\n",
    "        print(self.n_batches_all)\n",
    "        self.n_batches = self.m / self.batch_size\n",
    "        self.n_epochs = self.n_batches_all / self.n_batches\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            self.learning_rate_val = self.learning_rate_base\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    if(self.learning_rate_val < self.max_learning_rate):\n",
    "                        self.learning_rate_val += self.lr_gap\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                            self.input_x : batch_x,\n",
    "                            self.input_y : batch_y,\n",
    "                            self.learning_rate : self.learning_rate_val\n",
    "                        }\n",
    "                    print(feed_dict)\n",
    "                    #sess.run(self.train_step, feed_dict = feed_dict)\n",
    "    \n",
    "\n",
    "        \n",
    "lrf = lr_find(np.shape(train_x)[1], 5, np.array(embedding_matrix))\n",
    "lrf.build()\n",
    "lrf.fit(train_x, train_y)\n",
    "lrf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
