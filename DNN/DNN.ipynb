{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    156060.00000\n",
       "mean          6.89463\n",
       "std           6.57485\n",
       "min           0.00000\n",
       "25%           2.00000\n",
       "50%           4.00000\n",
       "75%           9.00000\n",
       "max          48.00000\n",
       "Name: phracelen, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './Tomatoes/'\n",
    "\n",
    "data = pd.read_csv(\"%s%s\" % (data_path, 'train.tsv'), sep = '\\t')\n",
    "test_data = pd.read_csv(\"%s%s\" % (data_path, 'test.tsv'), sep = '\\t')\n",
    "\n",
    "import re\n",
    "def clean(_str):\n",
    "    return \" \".join(re.findall(\"[0-9a-zA-Z]*\", _str)).strip()\n",
    "def split(_str):\n",
    "    return _str.split()\n",
    "\n",
    "data['Phrase'] = data['Phrase'].apply(clean)\n",
    "test_data['Phrase'] = data['Phrase'].apply(clean)\n",
    "\n",
    "def _len(_str):\n",
    "    return len(_str.split())\n",
    "data['phracelen'] = data['Phrase'].apply(_len)\n",
    "data['phracelen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124848, 5), (31212, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, train_size = 0.8, random_state = 22)\n",
    "\n",
    "for train_index, dev_index in split.split(data, data[['Sentiment']]):\n",
    "    dev_data = data.loc[dev_index]\n",
    "    train_data = data.loc[train_index]\n",
    "train_data.shape, dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir_2t/wangchenguang/anaconda3/envs/gpu-tf/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_w2v(splited_corpus, w2v_size, min_count):\n",
    "    '''\n",
    "    func: 获取word2vec模型\n",
    "    param: splited_corpus\n",
    "        type: pd.Series\n",
    "        detail: 应当为训练集中所有语料\n",
    "    param: w2v_size\n",
    "        type: int\n",
    "        detail: w2v向量维度\n",
    "    return: w2v_model\n",
    "        type: gensim.models.Word2Vec\n",
    "        detail: 训练的模型只可以使用其transform接口\n",
    "    '''\n",
    "    sentences = [x.split() for x in splited_corpus]\n",
    "    model = gensim.models.Word2Vec(sentences, min_count=min_count, size=w2v_size)\n",
    "    return model\n",
    "\n",
    "def get_w2v_key_vev(w2v_model):\n",
    "    vecs = []\n",
    "    words = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        vecs.append(w2v_model[word])\n",
    "        words.append(word)\n",
    "    return words, vecs\n",
    "\n",
    "def get_x_index(x, words):\n",
    "    res = []\n",
    "    for inst in x:\n",
    "        res.append(np.array([words.index(word) for word in inst.split() if word in words]))\n",
    "    return res\n",
    "\n",
    "def max_len(list_2d):\n",
    "    maxlen = 0\n",
    "    for arr in list_2d:\n",
    "        if(len(arr) > maxlen):\n",
    "            maxlen = len(arr)\n",
    "    return maxlen\n",
    "\n",
    "def mean_len(list_2d):\n",
    "    mean_len = 0\n",
    "    for arr in list_2d:\n",
    "        mean_len += len(arr)\n",
    "    return int(mean_len / len(list_2d))\n",
    "\n",
    "def ceil2(num):\n",
    "    res = 2\n",
    "    while res < num:\n",
    "        res *= 2\n",
    "    return res\n",
    "\n",
    "def padding(data2d, max_len, pad_val):\n",
    "    res = []\n",
    "    for index, seq in enumerate(data2d):\n",
    "        if(len(seq) < max_len):\n",
    "            res.append(np.concatenate([seq, np.full([max_len - len(seq)], pad_val)]))\n",
    "        else:\n",
    "            res.append(seq[:max_len])\n",
    "    return res\n",
    "\n",
    "def concat_list_h(list1, list2):\n",
    "    res = []\n",
    "    for i, ele in enumerate(list1):\n",
    "        res.append(np.concatenate([ele, list2[i]]))\n",
    "    return res\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()\n",
    "train_y = np.array(list(train_data['Sentiment'])).reshape(-1, 1)\n",
    "train_y = oh_enc.fit_transform(train_y).toarray()\n",
    "dev_y = np.array(list(dev_data['Sentiment'])).reshape(-1, 1)\n",
    "dev_y = oh_enc.fit_transform(dev_y).toarray()\n",
    "corpus = list(train_data['Phrase'])\n",
    "max_seq_len = ceil2(max_len(corpus))\n",
    "mean_seq_len = mean_len(corpus)\n",
    "print(max_seq_len, mean_seq_len)\n",
    "max_seq_len = 16\n",
    "w2v_model = get_w2v(corpus, 300, min_count = 1)\n",
    "words, embedding_matrix = get_w2v_key_vev(w2v_model)\n",
    "embedding_matrix.append([0 for i in range(len(embedding_matrix[0]))])\n",
    "\n",
    "train_x = get_x_index(list(train_data['Phrase']), words)\n",
    "train_x = padding(train_x, max_seq_len, len(embedding_matrix) - 1)\n",
    "\n",
    "dev_x = get_x_index(list(dev_data['Phrase']), words)\n",
    "dev_x = padding(dev_x, max_seq_len, len(embedding_matrix) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf log dir :  tf_logs/run-20191117031328/\n",
      "weight_0 0.46370396\n",
      "weight_1 0.4607172\n",
      "weight_2 0.4630477\n",
      "weight_3 0.46335676\n",
      "weight_4 0.45703107\n",
      "weight_5 0.43493444\n",
      "train_cross_e 3.3992834\n",
      "dev acc 0.53123796\n",
      "weight_0 0.3144973\n",
      "weight_1 0.31349128\n",
      "weight_2 0.31724837\n",
      "weight_3 0.31998953\n",
      "weight_4 0.3286518\n",
      "weight_5 0.40843377\n",
      "train_cross_e 2.1654773\n",
      "dev acc 0.54575163\n",
      "weight_0 0.21536334\n",
      "weight_1 0.21674477\n",
      "weight_2 0.22193377\n",
      "weight_3 0.22577414\n",
      "weight_4 0.24666291\n",
      "weight_5 0.3930574\n",
      "train_cross_e 1.6125463\n",
      "dev acc 0.5512303\n",
      "weight_0 0.15119484\n",
      "weight_1 0.1536245\n",
      "weight_2 0.15876354\n",
      "weight_3 0.16343723\n",
      "weight_4 0.19857872\n",
      "weight_5 0.40946534\n",
      "train_cross_e 1.365466\n",
      "dev acc 0.55145454\n",
      "weight_0 0.111814216\n",
      "weight_1 0.11285579\n",
      "weight_2 0.116992846\n",
      "weight_3 0.12372551\n",
      "weight_4 0.17320837\n",
      "weight_5 0.4227554\n",
      "train_cross_e 1.2604624\n",
      "dev acc 0.5512623\n",
      "weight_0 0.09025104\n",
      "weight_1 0.08761307\n",
      "weight_2 0.09001388\n",
      "weight_3 0.09953391\n",
      "weight_4 0.16110593\n",
      "weight_5 0.4289851\n",
      "train_cross_e 1.2059938\n",
      "dev acc 0.55379343\n",
      "weight_0 0.08048745\n",
      "weight_1 0.07303812\n",
      "weight_2 0.07332164\n",
      "weight_3 0.0856355\n",
      "weight_4 0.15617071\n",
      "weight_5 0.4346049\n",
      "train_cross_e 1.1860049\n",
      "dev acc 0.5546264\n",
      "weight_0 0.07731947\n",
      "weight_1 0.06529651\n",
      "weight_2 0.06336012\n",
      "weight_3 0.078244254\n",
      "weight_4 0.1543432\n",
      "weight_5 0.43853062\n",
      "train_cross_e 1.1914123\n",
      "dev acc 0.5486672\n",
      "weight_0 0.07696435\n",
      "weight_1 0.06137665\n",
      "weight_2 0.057528034\n",
      "weight_3 0.07461179\n",
      "weight_4 0.15395513\n",
      "weight_5 0.4415372\n",
      "train_cross_e 1.1722693\n",
      "dev acc 0.54930794\n",
      "weight_0 0.07788439\n",
      "weight_1 0.05946296\n",
      "weight_2 0.054208178\n",
      "weight_3 0.072924085\n",
      "weight_4 0.15426481\n",
      "weight_5 0.44642317\n",
      "train_cross_e 1.1615561\n",
      "dev acc 0.5519031\n",
      "weight_0 0.07889765\n",
      "weight_1 0.058418468\n",
      "weight_2 0.052216332\n",
      "weight_3 0.0722414\n",
      "weight_4 0.15465769\n",
      "weight_5 0.44954795\n",
      "train_cross_e 1.1594894\n",
      "dev acc 0.55119824\n",
      "weight_0 0.07971669\n",
      "weight_1 0.057801113\n",
      "weight_2 0.050995532\n",
      "weight_3 0.07223374\n",
      "weight_4 0.15485299\n",
      "weight_5 0.4530114\n",
      "train_cross_e 1.1664691\n",
      "dev acc 0.5471934\n",
      "weight_0 0.08056476\n",
      "weight_1 0.05742091\n",
      "weight_2 0.05028238\n",
      "weight_3 0.0724766\n",
      "weight_4 0.15549314\n",
      "weight_5 0.456671\n",
      "train_cross_e 1.1546094\n",
      "dev acc 0.55167884\n",
      "weight_0 0.08108607\n",
      "weight_1 0.057058714\n",
      "weight_2 0.049693424\n",
      "weight_3 0.07262764\n",
      "weight_4 0.15570857\n",
      "weight_5 0.46147728\n",
      "train_cross_e 1.1655161\n",
      "dev acc 0.551006\n",
      "weight_0 0.08146031\n",
      "weight_1 0.056829624\n",
      "weight_2 0.049232498\n",
      "weight_3 0.07289923\n",
      "weight_4 0.15598516\n",
      "weight_5 0.4651863\n",
      "train_cross_e 1.1851087\n",
      "dev acc 0.54664874\n",
      "weight_0 0.08167986\n",
      "weight_1 0.056367144\n",
      "weight_2 0.048724946\n",
      "weight_3 0.07302745\n",
      "weight_4 0.15633745\n",
      "weight_5 0.46781442\n",
      "train_cross_e 1.1684166\n",
      "dev acc 0.54693705\n",
      "weight_0 0.08176779\n",
      "weight_1 0.055937402\n",
      "weight_2 0.048287537\n",
      "weight_3 0.07316165\n",
      "weight_4 0.15696482\n",
      "weight_5 0.47236508\n",
      "train_cross_e 1.1850228\n",
      "dev acc 0.5478662\n",
      "weight_0 0.0818415\n",
      "weight_1 0.055770487\n",
      "weight_2 0.04804893\n",
      "weight_3 0.07349017\n",
      "weight_4 0.15780495\n",
      "weight_5 0.4741278\n",
      "train_cross_e 1.1670569\n",
      "dev acc 0.55071765\n",
      "weight_0 0.081799105\n",
      "weight_1 0.055440284\n",
      "weight_2 0.047690276\n",
      "weight_3 0.073594615\n",
      "weight_4 0.15871647\n",
      "weight_5 0.47890046\n",
      "train_cross_e 1.1583489\n",
      "dev acc 0.55408174\n",
      "weight_0 0.081654675\n",
      "weight_1 0.054969985\n",
      "weight_2 0.047252495\n",
      "weight_3 0.07371666\n",
      "weight_4 0.15920752\n",
      "weight_5 0.48028797\n",
      "train_cross_e 1.1773455\n",
      "dev acc 0.5501089\n",
      "weight_0 0.08153551\n",
      "weight_1 0.054677997\n",
      "weight_2 0.046933807\n",
      "weight_3 0.07406183\n",
      "weight_4 0.15956056\n",
      "weight_5 0.48013943\n",
      "train_cross_e 1.1490663\n",
      "dev acc 0.5546264\n",
      "weight_0 0.081420965\n",
      "weight_1 0.054469287\n",
      "weight_2 0.04659345\n",
      "weight_3 0.074376754\n",
      "weight_4 0.15974043\n",
      "weight_5 0.48284355\n",
      "train_cross_e 1.1704855\n",
      "dev acc 0.5506536\n",
      "weight_0 0.08148746\n",
      "weight_1 0.054212496\n",
      "weight_2 0.04634501\n",
      "weight_3 0.07451425\n",
      "weight_4 0.16041392\n",
      "weight_5 0.48315158\n",
      "train_cross_e 1.1472337\n",
      "dev acc 0.5541779\n",
      "weight_0 0.081634544\n",
      "weight_1 0.053987827\n",
      "weight_2 0.046039313\n",
      "weight_3 0.07459675\n",
      "weight_4 0.16055146\n",
      "weight_5 0.48376334\n",
      "train_cross_e 1.1712816\n",
      "dev acc 0.5505895\n",
      "weight_0 0.08152179\n",
      "weight_1 0.053718243\n",
      "weight_2 0.045723323\n",
      "weight_3 0.07499753\n",
      "weight_4 0.16087891\n",
      "weight_5 0.4861001\n",
      "train_cross_e 1.1762283\n",
      "dev acc 0.5528002\n",
      "weight_0 0.081381746\n",
      "weight_1 0.053472236\n",
      "weight_2 0.045496613\n",
      "weight_3 0.075143665\n",
      "weight_4 0.16173308\n",
      "weight_5 0.48771873\n",
      "train_cross_e 1.1710542\n",
      "dev acc 0.5509099\n",
      "weight_0 0.08149806\n",
      "weight_1 0.05317613\n",
      "weight_2 0.04520111\n",
      "weight_3 0.0753727\n",
      "weight_4 0.16221564\n",
      "weight_5 0.492159\n",
      "train_cross_e 1.1700422\n",
      "dev acc 0.55264\n",
      "weight_0 0.081596784\n",
      "weight_1 0.05306979\n",
      "weight_2 0.04499125\n",
      "weight_3 0.07559881\n",
      "weight_4 0.16275735\n",
      "weight_5 0.49346015\n",
      "train_cross_e 1.1619983\n",
      "dev acc 0.5561002\n",
      "weight_0 0.081525736\n",
      "weight_1 0.052918408\n",
      "weight_2 0.04472364\n",
      "weight_3 0.07592193\n",
      "weight_4 0.1630611\n",
      "weight_5 0.4930379\n",
      "train_cross_e 1.2131164\n",
      "dev acc 0.5472254\n",
      "weight_0 0.0813519\n",
      "weight_1 0.052599825\n",
      "weight_2 0.04436511\n",
      "weight_3 0.07581877\n",
      "weight_4 0.16346021\n",
      "weight_5 0.4918924\n",
      "train_cross_e 1.1768426\n",
      "dev acc 0.5488914\n",
      "weight_0 0.08114303\n",
      "weight_1 0.052433603\n",
      "weight_2 0.044124156\n",
      "weight_3 0.076139\n",
      "weight_4 0.16445802\n",
      "weight_5 0.49638045\n",
      "train_cross_e 1.1611426\n",
      "dev acc 0.55523515\n",
      "weight_0 0.08110629\n",
      "weight_1 0.05227724\n",
      "weight_2 0.04391553\n",
      "weight_3 0.07637263\n",
      "weight_4 0.16469853\n",
      "weight_5 0.49614066\n",
      "train_cross_e 1.1507926\n",
      "dev acc 0.5574779\n",
      "weight_0 0.08088026\n",
      "weight_1 0.052036192\n",
      "weight_2 0.043641202\n",
      "weight_3 0.07635183\n",
      "weight_4 0.1650225\n",
      "weight_5 0.49425125\n",
      "train_cross_e 1.1425401\n",
      "dev acc 0.55898374\n",
      "weight_0 0.08053396\n",
      "weight_1 0.051831048\n",
      "weight_2 0.043361146\n",
      "weight_3 0.07647443\n",
      "weight_4 0.1647991\n",
      "weight_5 0.4977367\n",
      "train_cross_e 1.1465762\n",
      "dev acc 0.55594003\n",
      "weight_0 0.080492504\n",
      "weight_1 0.05162502\n",
      "weight_2 0.043216962\n",
      "weight_3 0.07643805\n",
      "weight_4 0.1651326\n",
      "weight_5 0.4973854\n",
      "train_cross_e 1.1371953\n",
      "dev acc 0.5612264\n",
      "weight_0 0.08052877\n",
      "weight_1 0.051607996\n",
      "weight_2 0.043074626\n",
      "weight_3 0.07656313\n",
      "weight_4 0.16495176\n",
      "weight_5 0.5009764\n",
      "train_cross_e 1.1428099\n",
      "dev acc 0.5602012\n",
      "weight_0 0.08047622\n",
      "weight_1 0.051508643\n",
      "weight_2 0.042887628\n",
      "weight_3 0.07652913\n",
      "weight_4 0.16492169\n",
      "weight_5 0.50264406\n",
      "train_cross_e 1.1680044\n",
      "dev acc 0.55324876\n",
      "weight_0 0.080492266\n",
      "weight_1 0.051393867\n",
      "weight_2 0.042640477\n",
      "weight_3 0.076690905\n",
      "weight_4 0.16463917\n",
      "weight_5 0.5014927\n",
      "train_cross_e 1.1397008\n",
      "dev acc 0.56000894\n",
      "weight_0 0.08039089\n",
      "weight_1 0.051387824\n",
      "weight_2 0.042469144\n",
      "weight_3 0.07663591\n",
      "weight_4 0.16429436\n",
      "weight_5 0.5003555\n",
      "train_cross_e 1.1339905\n",
      "dev acc 0.5626682\n",
      "weight_0 0.08010596\n",
      "weight_1 0.051345743\n",
      "weight_2 0.042424515\n",
      "weight_3 0.076712534\n",
      "weight_4 0.16428661\n",
      "weight_5 0.49794906\n",
      "train_cross_e 1.1313772\n",
      "dev acc 0.5614828\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class DNNClassifier:\n",
    "    def __init__(self, neurons, input_dim, n_class, embedding_matrix, seq_len, learning_rate = 0.1, \n",
    "                 batch_size = 128, n_epochs = 500, hiden_act_fn = tf.sigmoid, output_act_fn = None, \n",
    "                 sum_root_dir = \"tf_logs\", embedding_drop_out_prob = 0.5, use_BN = False, rate_decay_steps = 100000,\n",
    "                 rate_decay_rate = 0.99, use_L2 = False, regulation_rate = 0.0001):\n",
    "        self.input_dim = input_dim\n",
    "        self.n_class = n_class\n",
    "        self.neurons = neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seq_len = seq_len\n",
    "        self.log_dir = self.log_dir(sum_root_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.hiden_act_fn = hiden_act_fn\n",
    "        self.output_act_fn = output_act_fn\n",
    "        self.hiden_layer_num = len(neurons)\n",
    "        self.embedding_drop_out_prob = embedding_drop_out_prob\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.use_BN = use_BN\n",
    "        self.rate_decay_steps = rate_decay_steps\n",
    "        self.rate_decay_rate = rate_decay_rate\n",
    "        self.use_L2 = use_L2\n",
    "        self.regulation_rate = regulation_rate\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "    def log_dir(self, root_logdir):\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "        return log_dir\n",
    "    \n",
    "    def neuron_layer(self, x, neuron_num, activate_fn, scope, is_trainning, is_use_BN, regularizer = None):\n",
    "        with tf.name_scope(scope):\n",
    "            n_input = int(x.shape[1])\n",
    "            weight = tf.Variable(tf.random_uniform([n_input, neuron_num], -1, 1), name = \"weight\")\n",
    "            bias = tf.Variable(tf.random_uniform([neuron_num], -1, 1), name = \"bias\")\n",
    "            output = tf.matmul(x, weight) + bias\n",
    "            if(is_use_BN):\n",
    "                scale = tf.Variable(tf.ones([neuron_num]), \n",
    "                                       name = \"scale\")\n",
    "                offset = tf.Variable(tf.zeros([neuron_num]),\n",
    "                                    name = \"offset\")\n",
    "                untrain_mean = tf.Variable(tf.zeros([neuron_num]), trainable = False,\n",
    "                                  name = \"untrainable_mean\")\n",
    "                untrain_variance = tf.Variable(tf.ones([neuron_num]), trainable = False,\n",
    "                                      name = \"untrainable_var\")\n",
    "                def train_bn():\n",
    "                    mean, variance = tf.nn.moments(output, [0])\n",
    "                    return tf.nn.batch_normalization(output, mean, variance,\n",
    "                                                    offset, scale, \n",
    "                                                     variance_epsilon = 0.01,\n",
    "                                                     name = \"bn_output_train\")\n",
    "                def inference_bn():\n",
    "                    return tf.nn.batch_normalization(output, untrain_mean, \n",
    "                                                     untrain_variance,\n",
    "                                                    offset, scale,\n",
    "                                                     variance_epsilon = 0.01,\n",
    "                                                     name = \"bn_output_train\")\n",
    "                output = tf.cond(is_trainning, train_bn, inference_bn)\n",
    "            if(activate_fn):\n",
    "                output = activate_fn(output)\n",
    "            else:\n",
    "                output = output\n",
    "            if(regularizer):\n",
    "                return weight, bias, output, regularizer(weight) + regularizer(bias)\n",
    "            else:\n",
    "                return weight, bias, output, None\n",
    "    \n",
    "    def build(self):\n",
    "        with self.graph.as_default(), tf.name_scope(\"DNN\"):\n",
    "            self.input_x = tf.placeholder(dtype = tf.int32, shape = [None, self.seq_len], name = \"input_x\")\n",
    "            self.input_y = tf.placeholder(dtype = tf.int32, shape = [None, self.n_class], name = \"input_y\")\n",
    "            self.emb_dropout_keep_prob = tf.placeholder(tf.float32, name=\"emb_dropout_keep_prob\")\n",
    "            self.global_step = tf.Variable(0, trainable = False)\n",
    "            self.learning_rate_ = tf.train.exponential_decay(\n",
    "                self.learning_rate,\n",
    "                self.global_step,\n",
    "                self.rate_decay_steps,\n",
    "                self.rate_decay_rate\n",
    "            )\n",
    "            if(self.use_L2):\n",
    "                self.regularizer = tf.contrib.layers.l2_regularizer(self.regulation_rate)\n",
    "            else:\n",
    "                self.regularizer = None\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.embedding_matrix_ = tf.constant(self.embedding_matrix, name = \"embedding_matrix\", dtype = tf.float32)\n",
    "                self.embedded = tf.nn.embedding_lookup(self.embedding_matrix_, self.input_x)\n",
    "                self.embedded = tf.reduce_mean(self.embedded, axis = 1)\n",
    "            with tf.name_scope(\"emb_bn\"):\n",
    "                emb_scale = tf.Variable(tf.ones([self.embedded.shape[1]]), \n",
    "                                       name = \"scale\")\n",
    "                emb_offset = tf.Variable(tf.zeros([self.embedded.shape[1]]),\n",
    "                                    name = \"offset\")\n",
    "                emb_mean, emb_variance = tf.nn.moments(self.embedded, [0])\n",
    "                self.bn_embedded = tf.nn.batch_normalization(self.embedded, emb_mean, emb_variance,\n",
    "                                                emb_offset, emb_scale, \n",
    "                                                 variance_epsilon = 0.01,\n",
    "                                                 name = \"bn_embedded\")\n",
    "            with tf.name_scope(\"emb_dropout\"):\n",
    "                #使用初始BN后不使用dropout\n",
    "                #self.droped_embedded = tf.nn.dropout(self.embedded, self.emb_dropout_keep_prob)\n",
    "                #self.droped_embedded = tf.nn.dropout(self.bn_embedded, self.emb_dropout_keep_prob)\n",
    "                self.droped_embedded = self.bn_embedded\n",
    "            with tf.name_scope(\"global\"):\n",
    "                self.is_trainning = tf.placeholder(tf.bool)\n",
    "            '''\n",
    "            隐藏层与输出层的建立与连接\n",
    "            '''\n",
    "            self.neurons.append(self.n_class)\n",
    "            self.layer_weights = []\n",
    "            self.layer_biases = []\n",
    "            self.layer_outputs = []\n",
    "            for layer_index in range(len(self.neurons)):\n",
    "                if(0 == layer_index):\n",
    "                    layer_input = self.droped_embedded\n",
    "                else:\n",
    "                    layer_input = self.layer_outputs[-1]\n",
    "                if(layer_index == len(self.neurons) - 1):\n",
    "                    act_fn = self.output_act_fn\n",
    "                else:\n",
    "                    act_fn = self.hiden_act_fn\n",
    "                weight, bias, output, regulation_ = self.neuron_layer(layer_input, self.neurons[layer_index],\n",
    "                                                        act_fn, scope = \"hiden_layer_\" + str(layer_index),\n",
    "                                                        is_trainning = self.is_trainning, is_use_BN = self.use_BN,\n",
    "                                                        regularizer = self.regularizer)\n",
    "                self.layer_weights.append(weight)\n",
    "                self.layer_biases.append(bias)\n",
    "                self.layer_outputs.append(output)\n",
    "                if(self.use_L2):\n",
    "                    if(layer_index == 0):\n",
    "                        self.regulation = regulation_\n",
    "                    else:\n",
    "                        self.regulation += regulation_\n",
    "            \n",
    "            self.prediction = self.layer_outputs[-1]\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_e = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits = self.prediction, labels = self.input_y))\n",
    "                if(self.use_L2):\n",
    "                     self.cross_e += self.regulation\n",
    "            with tf.name_scope(\"target\"):\n",
    "                self.correct = tf.equal(tf.argmax(self.prediction, 1), tf.arg_max(\n",
    "                    self.input_y, 1), name = 'correct')\n",
    "                self.acc = tf.reduce_mean(tf.cast(self.correct, tf.float32), name = 'acc')\n",
    "            #with tf.name_scope(\"saver\"):\n",
    "                #self.saver = tf.train.Saver()\n",
    "            with tf.name_scope(\"summary\"):\n",
    "                self.crosse_summary = tf.summary.scalar(\"cross_e\", self.cross_e)\n",
    "                self.acc_summary = tf.summary.scalar(\"acc\", self.acc)\n",
    "                self.weights_mean = []\n",
    "                self.weights_mean_summary = []\n",
    "                for layer_index in range(len(self.neurons)):\n",
    "                    self.weights_mean.append(\n",
    "                        tf.reduce_mean(tf.sqrt(tf.square(self.layer_weights[layer_index]))))\n",
    "                    self.weights_mean_summary.append(\n",
    "                        tf.summary.scalar(\"weight_magnitude_\" + str(layer_index), \n",
    "                                          self.weights_mean[-1]))\n",
    "                self.filewriter = tf.summary.FileWriter(self.log_dir, tf.get_default_graph())\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate = \n",
    "                                                         self.learning_rate_)\n",
    "            self.train_step = self.optimizer.minimize(self.cross_e, global_step = self.global_step)\n",
    "                \n",
    "    def fit(self, x, y, dev_x = None, dev_y = None, test_x = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.dev_x = dev_x\n",
    "        self.dev_y = dev_y\n",
    "        self.test_x = test_x\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.sess.run(self.prediction, feed_dict = {\n",
    "                        self.input_x : self.x_dev,\n",
    "                        self.is_trainning : False\n",
    "                    })\n",
    "    \n",
    "    def fetch_batch(self, X, Y, batch_index, batch_size):\n",
    "        return X[batch_index * batch_size : batch_index * batch_size + batch_size], Y[batch_index * batch_size : batch_index * batch_size + batch_size]\n",
    "    \n",
    "    def train(self):\n",
    "        print('tf log dir : ', self.log_dir)\n",
    "        n_batches = int(np.ceil(len(self.x) / self.batch_size))\n",
    "        batch_size = self.batch_size\n",
    "        dev_feed_dict = {\n",
    "            self.input_x : self.dev_x,\n",
    "            self.input_y : self.dev_y,\n",
    "            self.emb_dropout_keep_prob : 1.0,\n",
    "            self.is_trainning : False\n",
    "        }\n",
    "        train_feed_dict = {\n",
    "            self.input_x : self.x[:10000],\n",
    "            self.input_y : self.y[:10000],\n",
    "            self.emb_dropout_keep_prob : 1.0,\n",
    "            self.is_trainning : False\n",
    "        }\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        with tf.Session(graph = self.graph, config = config) as self.sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    batch_x = self.x[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    batch_y = self.y[batch_size * batch_index : (batch_index + 1) * batch_size]\n",
    "                    feed_dict = {\n",
    "                        self.input_x : batch_x,\n",
    "                        self.input_y : batch_y,\n",
    "                        self.emb_dropout_keep_prob : self.embedding_drop_out_prob,\n",
    "                        self.is_trainning : True\n",
    "                    }\n",
    "                    self.sess.run(self.train_step, feed_dict = feed_dict)\n",
    "                    step = epoch * n_batches + batch_index\n",
    "                if(epoch % 5 == 0):\n",
    "                    for layer_index in range(len(self.neurons)):\n",
    "                        print('weight_' + str(layer_index),\n",
    "                            self.weights_mean[layer_index].eval(feed_dict = train_feed_dict))\n",
    "                    print('train_cross_e', self.cross_e.eval(feed_dict = train_feed_dict))\n",
    "                    print('dev acc', self.acc.eval(feed_dict = dev_feed_dict))\n",
    "                    #self.saver.save(sess, \"DNN.ckpt\")\n",
    "                    crosse_summary_str = self.crosse_summary.eval(feed_dict = train_feed_dict)\n",
    "                    acc_summary_str = self.acc_summary.eval(feed_dict = dev_feed_dict)\n",
    "                    self.filewriter.add_summary(crosse_summary_str, step)\n",
    "                    self.filewriter.add_summary(acc_summary_str, step)\n",
    "                    for layer_index in range(len(self.neurons)):\n",
    "                        weight_mean_str = self.weights_mean_summary[layer_index].eval(feed_dict = train_feed_dict)\n",
    "                        self.filewriter.add_summary(weight_mean_str, step)\n",
    "            self.filewriter.close()           \n",
    "               \n",
    "m = DNNClassifier([250, 200, 100, 80, 20], len(embedding_matrix[0]), 5, np.array(embedding_matrix), max_seq_len,\n",
    "                  learning_rate = 0.8, n_epochs= 200, hiden_act_fn = tf.sigmoid,\n",
    "                 output_act_fn = None, rate_decay_steps = 1000000, rate_decay_rate = 0.99, use_L2 = True)\n",
    "m.build()\n",
    "\n",
    "m.fit(train_x, train_y, dev_x, dev_y)\n",
    "m.train()\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
